# Anycall Frontend

<img width="921" height="649" alt="스크린샷 2026-01-13 오후 5 59 45" src="https://github.com/user-attachments/assets/2ee81aaa-4c09-40e7-8484-66002eaa7230" />


# 📞 산학프로젝트: Anycall

> **팀명**: 컴공으로 앞서강  
> **담당 교수**: 정영민 교수님  
> **팀원**: 20211558 윤준서, 20211506 김도영, 20211561 이규형, 20211576 이준수

---

## 📑 목차
1. [개요](#1-개요)
    - [1-1. 프로젝트 소개](#1-1-프로젝트-소개)
    - [1-2. 추진 배경 및 필요성](#1-2-추진-배경-및-필요성)
    - [1-3. 핵심 가치](#1-3-핵심-가치)
    - [1-4. 프로젝트 목표](#1-4-프로젝트-목표)
    - [1-5. 프로젝트 독창성](#1-5-프로젝트-독창성)
2. [개발 내용](#2-개발-내용)
    - [2-1. 전체 프로그램 구조도](#2-1-전체-프로그램-구조도)
    - [2-2. 설계 주요 고려사항](#2-2-설계-주요-고려사항)
    - [2-3. 개발 항목](#2-3-개발-항목)
    - [2-4. 이슈 및 해결방안](#2-4-이슈-및-해결방안)
3. [성과](#3-성과)
4. [검증](#4-검증)
5. [향후 발전 방향](#5-향후-발전-방향)
    - [5-1. 서비스 확장 가능성](#5-1-서비스-확장-가능성)
    - [5-2. 고도화 로드맵](#5-2-고도화-로드맵)
    - [5-3. 잠재적 사업화 방향](#5-3-잠재적-사업화-방향)

---

## 1. 개요

### 1-1. 프로젝트 소개
Anycall은 AI 음성 모델과 대화형 LLM을 기반으로, 시니어 사용자가 복잡한 조작 없이 자연스러운 음성 대화를 이어갈 수 있도록 설계된 가상 통화 서비스입니다. 특히 고령층·사회적 취약계층을 대상으로 실제 전화와 유사한 경험을 제공하는 것을 핵심 목표로 하며, 기존의 음성 기반 AI 서비스가 가진 접근성 문제를 개선합니다.

### 1-2. 추진 배경 및 필요성
현재 상용화된 대화형 AI 서비스는 UI 복잡성, 낮은 시니어 친화성, 지속적인 맥락 유지의 어려움 등으로 인해 고령층 사용자 만족도가 낮다는 문제가 있습니다. Anycall은 이러한 불편을 해소하기 위해 '통화 시작' 버튼만으로 바로 대화가 가능한 간단한 구조를 채택하고, Whisper 기반 STT·대규모 언어 모델·특정 인물 기반 TTS를 결합해 실제 전화에 가까운 상호작용 흐름을 구현합니다.

### 1-3. 핵심 가치
사용자의 계정을 기반으로 이전 대화 맥락을 기억하고 이어갈 수 있도록 설계함으로써 단순 질의응답을 넘어 지속적인 관계 형성과 감정적 안정 제공을 목표로 합니다. 이는 돌아가신 배우자나 가족의 목소리로 대화하고 싶은 고령층 심리적 니즈를 반영한 서비스 방향성과, 개인화된 음성 모델 기술의 발전이 맞물린 결과입니다.

### 1-4. 프로젝트 목표
Anycall은 일상 대화뿐 아니라 병원 예약, 생활 정보 안내, 120 상담 연계와 같은 공공 서비스 보완 역할까지 수행할 수 있도록 확장성을 갖추고 있습니다. 궁극적으로는 시니어가 일상에서 가장 익숙하게 사용하는 '전화'라는 인터페이스를 중심으로, AI 기반의 정서적 안정, 정보 접근성, 사용 편의성을 동시에 충족하는 지속 가능한 소통 플랫폼을 구축하는 것을 지향합니다.

### 1-5. 프로젝트 독창성
Anycall은 고령층 사용자 중심의 음성 기반 상호작용을 핵심으로 삼아, 기존의 AI 대화 서비스가 제공하지 못했던 정서적 안정·지속적 관계·개인화 음성 경험을 통합적으로 구현했다는 점에 있습니다. 단순히 음성으로 응답하는 챗봇 수준을 넘어, 주변 인물의 목소리를 사용하여 안정감을 높였습니다. 또한 실제 전화와 동일한 UX·UI를 제공함으로써 시니어 계층이 일상적으로 사용할 수 있는 현실적인 접근을 채택했습니다.


## 2. 개발 내용

### 2-1. 전체 프로그램 구조도
Anycall 서비스 음성 처리 흐름
* **1. [사용자 음성 입력]**
* **2. [잡음 제거 + 음성 인식(STT)]**
</br>– Whisper / Faster-Whisper / Whisper-Turbo
</br>– Google Speech API
* **3. [대화 생성 엔진(LLM)]**
</br>– GPT-4, Gemini, KoGPT
* **4. [음성 합성(TTS)]**
</br>– ElevenLabs
</br>– 커스텀 개인화 음성 모델
* **5. [모바일 앱 처리]**
</br>– Android Studio(Kotlin), Xcode 기반 실시간 재생
* **6. [백엔드 및 데이터베이스]**
</br>– FastAPI 기반 실시간 통신 처리
</br>– MongoDB / Firebase 저장 구조
* **7. [맥락 기억 모듈]**
</br>– Session Manager 기반 대화 맥락 저장 및 활용

### 2-2. 사용 기술
<img width="541" height="305" alt="스크린샷 2026-02-25 오후 7 40 34" src="https://github.com/user-attachments/assets/4d8270f4-35d5-4e99-ae16-299166dac94f" />


### 2-3. 개발 항목
* **2-3-1. STT-LLM-TTS 음성 파이프라인**
</br>전체 음성 파이프라인은 0.5초 이내 STT와 0.8초 이내 TTS 생성을 목표로 설계되었습니다. 한국어 특성을 반영해 Whisper v3를 한국어 데이터로 추가 학습하여 WER을 약 15% 개선하고 방언 대응력을 높였습니다.
</br>또한 VAD 기반 스트리밍 세그멘테이션을 적용해 발화 종료 후 약 0.3-0.5초 내 STT 결과를 반환하도록 하여 지연을 크게 줄였습니다. TTS는 문장 단위로 즉시 생성하는 스트리밍 방식을 도입해 3-5초의 대기 시간을 제거하고 0.3-0.8초 수준의 빠른 응답을 구현했습니다. 이러한 최적화를 통해 실시간 통화 환경에서도 끊김 없는 자연스러운 상호작용이 가능해졌습니다.

* **2-3-2. 대화 엔진 및 맥락 기억 구조**
</br>대화 엔진은 STT와 TTS 사이에서 사용자의 의도와 감정을 분석하고 자연스러운 응답을 생성하는 핵심 모듈로, 대화 맥락을 지속적으로 유지하도록 설계되었습니다. STT 텍스트와 이전 발화 기록을 함께 분석하여 문맥 기반 응답을 생성하며, LLM을 활용해 긴 대화 흐름과 사용자 말투·선호를 반영할 수 있도록 했습니다.
</br>또한 사용자 계정 단위로 대화 로그를 저장해 이전 주제를 이어가는 맥락 기반 대화가 가능하도록 구현했습니다. 상황별 프롬프트 템플릿을 적용해 이해하기 쉬운 표현을 사용하도록 했고, 응답 길이와 어투를 자동 조절하는 후처리와 필터링을 통해 안정적이고 신뢰성 있는 대화를 제공하도록 했습니다.

* **2-3-3. 모바일 앱(UI/UX 및 기능 흐름)**
</br>모바일 애플리케이션은 시니어 사용자가 복잡한 조작 없이 AI와 바로 대화할 수 있도록 단순한 UI로 설계되었습니다. 앱 실행 시 ‘통화 시작’ 버튼만으로 즉시 연결되며, 통화 화면에는 상태 표시와 종료 버튼만 배치해 불필요한 요소를 제거했습니다.
</br>음성은 WebSocket 기반 스트리밍 구조로 송수신되어, 사용자의 발화 직후 AI 응답이 자연스럽게 재생되도록 지연을 최소화했습니다. 또한 큰 글씨, 높은 대비 색상, 직관적인 버튼 배치를 적용해 실제 전화 통화와 유사한 간편한 사용 경험을 제공했습니다.

* **2-3-4. 실시간 통신 기반 백엔드 아키텍처**
</br>백엔드 시스템은 STT–LLM–TTS 파이프라인이 지연 없이 동작하도록 낮은 지연과 높은 처리량을 동시에 충족하는 구조로 설계되었습니다. 핵심은 비동기 WebSocket 기반의 Non-Blocking I/O 통신으로, 대규모 동시 연결에서도 안정적인 실시간 음성 스트리밍이 가능하도록 구현했습니다.
</br>또한 문자열 기반 프로토콜 대신 순수 이진데이터 전송 방식을 사용해 직렬화·파싱 비용과 네트워크 오버헤드를 최소화했으며, 1바이트 인덱스 프로토콜을 도입해 데이터 유형을 즉시 구분하도록 했습니다. 이를 통해 지연 시간을 크게 줄이고, 실제 통화 환경에서도 끊김 없는 상호작용과 높은 성능·확장성을 확보했습니다.

### 2-4. 이슈 및 해결방안
* **2-4-1. 기술적 리스크**
* **2-4-2. 데이터·모델·UX 관련 리스크**


## 3. 성과
* **3-1. 프로젝트 산출물**
</br>기획 단계에서부터 산출물 중심의 개발 방식을 채택하였으며, 기능 개발·모델 학습·서버 구축·UX 설계 등 전 과정에서 문서와 결과물을 분리해 체계적으로 관리했습니다. 산출물은 설계 문서, 구현 결과물, 테스트 자료 등 총 6개 항목으로 구성됩니다.
</br></br>모바일 애플리케이션(Android)은 시니어 친화적 UI/UX를 기반으로 한 프로토타입을 포함하고 있으며, 음성 입력(STT), 대화 생성(LLM), 음성 출력(TTS)을 통합한 실시간 통화 기능을 구현하였습니다. 또한 기본 정보 조회와 공공 서비스 안내 기능도 화면 형태로 반영하였습니다.
</br></br>AI 모델 산출물에는 Whisper 기반 STT 모델의 적용 결과와 성능 측정 데이터가 포함되었고, 특정 인물 기반의 TTS 음성 모델과 관련 음성 샘플 및 파라미터가 함께 제공되었습니다. Generative AI를 활용한 대화 구조, 프롬프트 템플릿, 그리고 맥락 유지 모듈도 별도 정리하여 모델 작동 방식과 구조적 특성을 문서화했습니다.
</br></br>서버와 인프라 관련 산출물에는 SpringBoot 기반 API 서버 구현본과 사용자 계정·대화 로그·음성 모델 설정 등을 포함한 데이터베이스 스키마 문서가 포함되었습니다. STT와 TTS 요청 처리 로직, 오류 처리 구조, 서버 성능 측정 결과도 함께 기록하여 시스템 안정성과 확장성을 검토할 수 있도록 했습니다.
</br></br>기획 및 설계 문서는 프로젝트 배경과 필요성, 목표, 기능 범위 등을 포함한 프로젝트 기획서와 STT–LLM–TTS 파이프라인 흐름을 기준으로 한 시스템 아키텍처 문서로 구성하였습니다. UI 흐름도, 사용자 시나리오, 서비스 플로우 차트 또한 포함하여 서비스 구조를 시각적으로 이해할 수 있도록 하였습니다.
</br></br>데이터 및 실험 자료에는 수집·정제된 음성 데이터셋과 STT·TTS 모델의 단계별 성능 분석표, 대화 생성 실험 로그, 오류 케이스 분석이 포함되어 모델 성능 검증 과정을 체계적으로 정리하였습니다. 
</br></br>마지막으로 운영 및 협업 문서에는 Kick-off 회의록과 기술 스택 결정 기록, 주간 회의 요약과 기능 진행 보고서, 팀 행동규범 및 의사소통 규칙 등 프로젝트 운영 과정에서 생성된 문서 전반이 포함되었습니다.


* **3-2. 멘토 평가 의견**
</br></br>진행 기간 동안 총 네 차례의 멘토링을 통해 방향성 점검과 기술적 조언을 받았습니다. 각 멘토링에서는 프로젝트의 강점과 개선 사항이 균형 있게 논의되었으며, 멘토는 전반적으로 프로젝트의 기획 완성도와 기술적 잠재력을 높이 평가하였습니다.
</br></br>먼저, 온보딩 멘토링(2025.11.1 진행)에서는 팀이 제시한 문제 정의와 서비스 목표가 명확하게 정리되어 있다는 점을 긍정적으로 평가해 주셨습니다. 특히 시니어층의 정서적 고립 문제를 기술적으로 해결하려는 시도는 사회적 가치가 크다고 언급하셨으며, 프로젝트의 기본 방향이 충분히 타당하다고 말씀해 주셨습니다. 초기 단계임에도 기획서 구성과 서비스 구성이 체계적이었다는 점에서 좋은 평가를 받았습니다.
</br></br>두 번째로 진행된 멘토링(2025.11.8 진행)에서는 타깃 사용자 범위 조정에 대한 조언을 받았습니다. 초기 아이디어가 10대와 시니어 두 계층을 모두 포함하고 있었으나, 멘토는 “두 타깃층의 필요가 매우 다르기 때문에 서비스 효과를 극대화하려면 우선 시니어에 집중하는 것이 더 전략적”이라고 제안하였습니다. 팀이 제안을 수용하고 타깃을 시니어 중심으로 축소하자, 멘토는 “결정이 빠르고 명확하여 기획 완성도가 더 높아졌다”고 칭찬해 주셨습니다.
</br></br>세 번째 멘토링(2025.11.16 진행)에서는 성능 발전 방향에 대한 기술적 조언이 이루어졌습니다. 특히 STT–LLM–TTS 파이프라인의 성능이 서비스 품질을 좌우하므로, STT의 환경 적응력 향상, TTS의 음색 자연스러움 개선, LLM의 문맥 유지 강화를 중심으로 발전시켜야 한다고 조언하셨습니다. 또한 “학부 수준에서 이 정도의 기술 통합을 시도하는 것만으로도 상당히 도전적인 성취이며, 구현된 결과도 기대 이상”이라는 칭찬도 함께 해주셨습니다. 모델 실험 기록과 비교 분석을 꾸준히 남기고 있다는 점도 좋은 평가를 받았습니다.
</br></br>마지막 멘토링(2025.11.23 진행)에서는 프로젝트 발표에 대한 조언이 이루어졌습니다. 멘토는 발표 시 기술적 구조만 강조하는 것보다, 시니어 사용자에게 어떤 실제 변화를 줄 수 있는지를 중심 메시지로 전달하는 것이 더 효과적이라고 조언하였습니다. 발표 자료 구성과 스토리텔링 흐름 역시 전반적으로 안정적이라고 평가하며, 서비스 완성도가 높아 실제 전시에서도 좋은 반응을 얻을 수 있을 것이라는 격려의 말씀도 덧붙이셨습니다.
</br></br>멘토님께서는 프로젝트의 문제 정의, 기술 구현 수준, 팀의 의사결정 속도와 방향성 등을 높게 평가하였으며, 본 프로젝트가 학기 프로젝트를 넘어 실제 서비스로 확장될 가능성이 충분하다는 긍정적인 의견을 제시해 주셨습니다.



## 4. 검증
* **4-1. 테스트 계획 및 기준**
</br>**테스트 목적**
</br>• 시니어 사용자가 ‘통화 시작’ 중심의 단순한 조작만으로 대화를 시작·종료할 수 있는지 확인
</br>• STT–LLM–TTS 음성 파이프라인이 실시간 대화에 필요한 지연 시간 목표를 충족하는지 확인
</br>• 통합 동작 및 예외 상황(네트워크/무음/오류)에서 서비스가 안정적으로 복구되는지 확인
</br></br>**테스트 범위**
</br>• 기능 테스트: 통화 시작/종료, 음성 입력 및 응답 재생, 대화 기록 저장·조회, 목소리 설정(음성 업로드/선택) 등 핵심 기능 흐름
</br>• 성능 테스트: STT 지연, TTS(음성 합성) 추론 지연 등 실시간성에 직결되는 구간
</br>• 통합 테스트: STT→LLM→TTS가 연속적으로 동작하는 1-turn/다-turn 시나리오(일상 대화, 정보 문의, 공공서비스 연계 안내 등)
</br>• UX 테스트: 시니어 관점에서의 조작성, 가독성, 이해 가능성(정성 피드백 중심)
</br></br>**평가 기준(핵심 KPI)**
</br>• STT 지연 시간: 사용자가 발화를 끝낸 시점부터 텍스트가 생성될 때까지 평균 0.5초 이내
</br>• TTS 추론 시간: 응답 텍스트 입력부터 음성 생성(첫 출력 준비)까지 평균 0.8초 이내
</br>• 안정성: 반복 실행 시 치명적 오류(앱 강제 종료/통화 불가)가 발생하지 않을 것

* **4-2. 테스트 결과 및 분석**
</br>테스트 결과, Anycall 서비스의 핵심 성능 목표(음성 인식 0.5초 이내, 음성 생성 0.8초 이내)를 전반적으로 충족하였으며, API 시그널링과 WebSocket을 결합한 통합 시나리오에서도 끊김 없는 양방향 데이터 송수신을 확인했습니다. 이를 통해 실시간 대화에 필수적인 응답성과 시스템 안정성을 모두 확보하였습니다.

* **TTS(음성 합성) 추론 성능**
</br>GPTSoVits V2 Pro 기준, 추론 시간은 평균 0.562193초로 측정되었으며(n=400), 목표(평균 0.8초 이내)를 만족했습니다. 
</br>목표 대비 0.237807초(약 29.7%)의 여유를 확보하여, 실시간 통화 환경에서도 문장 단위 스트리밍 출력이 가능하다는 점을 확인했습니다.

* **STT(음성 인식) 지연 성능**
</br>VAD(Voice Activity Detection) 기반 스트리밍 세그멘테이션을 적용한 결과, 사용자가 발화를 끝낸 후 약 0.3~0.5초 내 STT 결과를 반환할 수 있었습니다. 이는 설계 목표(평균 0.5초 이내)를 충족하며, 실시간 대화에서 체감 지연을 줄이는 데 유효했습니다.

* **시스템 통합 및 통신 안정성**
</br>REST API를 통한 세션 생성(Session ID 발급) 후, WebSocket으로 즉시 전환되는 하이브리드 통신 구조가 정상적으로 동작함을 확인했습니다. 로그 상에서 Client와 GPU Worker가 동일한 세션 ID를 통해 매핑되고 연결되는 과정이 검증되었습니다.
</br>고객(Client)의 음성 데이터와 GPU Worker의 AI 응답(Audio)이 바이너리 포맷으로 손실 없이 교환됨을 확인했습니다. 특히 1,024 bytes 단위의 청크 데이터가 양방향으로 지연 없이 전송됨에 따라, Full-Duplex 통신 환경에서의 안정성을 확보했습니다.

* **통합 파이프라인 관점 분석**
</br>STT(0.3~0.5초)와 TTS(평균 0.562초) 구간의 지연 목표를 달성함으로써, LLM 응답 생성 시간 변동이 있더라도 전체 대화 지연의 하한을 낮출 수 있었습니다.
</br>또한 문장 단위 스트리밍 TTS 방식을 적용하여, 응답 문장이 길어지는 경우에도 ‘응답이 완성될 때까지 기다리는 시간’을 줄여 사용자 경험을 개선했습니다.

* **기능 및 UX 관점 결과**
</br>통화 시작→발화→응답 재생→통화 종료의 핵심 플로우를 중심으로 기능 동작을 점검하였고, 베타 테스트를 통해 확인된 기능적 이슈를 보완하여 안정성을 강화했습니다.
</br>시니어 사용성 관점에서는 조작 요소를 최소화하고 높은 대비·큰 버튼 등 UX 원칙을 적용한 구성이 직관적이라는 정성적 피드백을 확인했습니다.

* **한계 및 보완 방향**
</br>LLM 응답 시간은 외부 API/네트워크 상태에 따라 변동 가능성이 있으므로, 향후에는 응답 길이 제어 및 스트리밍 응답 도입 등을 통해 체감 지연을 더 안정적으로 관리할 필요가 있습니다.
</br>다양한 발화(속도·억양·잡음) 조건과 더 많은 시니어 사용자를 대상으로 한 추가 검증을 통해 정확도 및 UX 품질을 지속적으로 개선할 계획입니다.


## 5. 향후 발전 방향

### 5-1. 서비스 확장 가능성
병원 예약, 복지 상담, 지역 행정 안내 등 공공 서비스 연계 기능을 강화할 경우 공공기관의 정보 전달 부담을 줄이는 보조 서비스로 자리잡을 수 있습니다. 
특히 120 다산콜센터, 보건소 등과의 API 연계를 확대한다면 시니어에게 실질적인 편의를 제공하는 중요한 서비스로 발전될 수 있습니다.

### 5-2. 고도화 로드맵
향후 서비스 고도화는 음성 파이프라인의 품질 향상과 사용자 경험 개선, 개인화 기능 확장, 그리고 보안·윤리 체계 강화 등을 중심으로 진행될 계획입니다.
* **음성 파이프라인**: Whisper 기반 STT의 환경 적응력을 높이고, TTS 음색의 자연스러움을 정교하게 다듬을 예정입니다.
* **LLM**: 문맥 유지 능력을 강화하여 실제 통화에 가까운 자연스러운 상호작용을 가능하게 합니다.
* **UX 강화**: 사용자 피드백을 반영해 버튼 배치, 화면 전환 속도, 글자 크기 등을 재검토하여 학습 없이 사용할 수 있는 환경을 목표로 합니다.
* **개인화 확장**: 감정 톤 조절, 말투 변화, 대화 스타일 맞춤화 기능 등을 고도화하여 정서적 안정과 친숙함을 강화할 예정입니다.

### 5-3. 잠재적 사업화 방향
시니어 돌봄 시장을 중심으로 B2C 서비스 모델로 확장될 수 있습니다. 1인 가구 증가와 고령화 심화로 인해 정서적·정보적 지원에 대한 수요가 꾸준히 증가하고 있어, 서비스 가입 기반의 모델로 안정적으로 운영될 수 있는 가능성이 있습니다.
